{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
    "from safetensors.torch import load_file\n",
    "from bergson.data import IndexConfig\n",
    "from bergson.approx_unrolling.utils import TensorDict\n",
    "import json\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "from datetime import timedelta\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from datasets import Dataset, IterableDataset\n",
    "from torch.distributed.fsdp import fully_shard\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig, PreTrainedModel\n",
    "\n",
    "from bergson.data import IndexConfig, allocate_batches, DataConfig\n",
    "from bergson.distributed import distributed_computing\n",
    "from bergson.gradients import GradientProcessor\n",
    "from bergson.hessians.covariance_all_factors import (\n",
    "    compute_covariance,\n",
    "    compute_eigendecomposition,\n",
    "    compute_eigenvalue_correction,\n",
    ")\n",
    "from bergson.utils import assert_type, get_layer_list\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = IndexConfig(run_path=\"\")  # empty run path because we are not using it to save data\n",
    "cfg.model = \"EleutherAI/Pythia-14m\"\n",
    "cfg.precision = \"fp16\"\n",
    "cfg.revision = None\n",
    "cfg.fsdp = False\n",
    "cfg.normalizer = \"none\"\n",
    "cfg.fisher_fourth_root = False\n",
    "cfg.data = DataConfig(\"NeelNanda/pile-10k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "match cfg.precision:\n",
    "    case \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    case \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    case \"fp32\":\n",
    "        dtype = torch.float32\n",
    "    case \"int4\" | \"int8\":\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    case other:\n",
    "        raise ValueError(f\"Unsupported precision: {other}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=(\n",
    "        BitsAndBytesConfig(\n",
    "            load_in_4bit=cfg.precision == \"int4\",\n",
    "            load_in_8bit=cfg.precision == \"int8\",\n",
    "            bnb_4bit_compute_dtype=dtype,\n",
    "            bnb_4bit_quant_storage=dtype,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        if cfg.precision in (\"int4\", \"int8\")\n",
    "        else None\n",
    "    ),\n",
    "    torch_dtype=dtype,\n",
    "    revision=cfg.revision,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
