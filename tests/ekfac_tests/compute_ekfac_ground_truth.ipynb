{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict, IterableDatasetDict, load_dataset\n",
    "from safetensors.torch import load_file, save_file\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from bergson.data import DataConfig, IndexConfig, pad_and_tensor, tokenize\n",
    "from bergson.gradients import (\n",
    "    GradientProcessor,\n",
    ")\n",
    "from bergson.hessians.collector import EkfacCollector\n",
    "from bergson.hessians.utils import TensorDict\n",
    "from bergson.utils import assert_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_batches_test(doc_lengths: list[int], N: int, workers: Optional[int] = None) -> list[list[list[int]]]:\n",
    "    \"\"\"\n",
    "    Modification of allocate_batches to return a flat list of batches for testing(instead of returning allocation[rank])\n",
    "    Allocate documents into batches that are then distributed evenly across\n",
    "    a fixed number of workers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lengths : Sequence[int]\n",
    "        Length (in tokens) of each document.  The *i-th* document is referred to\n",
    "        internally by its index ``i``.\n",
    "    workers : int\n",
    "        Number of parallel workers ( 1 ≤ workers ≤ 8).\n",
    "    N : int\n",
    "        Hard memory budget per *batch*, expressed as\n",
    "        ``max(length in batch) * (# docs in batch) ≤ N``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[list[int]]]\n",
    "        ``allocation[w][b]`` is the list of document indices that belong to the\n",
    "        *b-th* batch assigned to worker ``w``.  Every worker receives the same\n",
    "        number of (non-empty) batches.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AllocationError\n",
    "        If the three hard constraints cannot be satisfied.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1.  **Per-batch cost constraint**:  Each batch is padded to the maximum\n",
    "        sequence length *inside that batch*, so its cost in “token × examples”\n",
    "        units is ``max_len_in_batch * batch_size``.  This must stay ≤ ``N``.\n",
    "    2.  **Bin-packing strategy**:  We use *first-fit decreasing* (FFD) to obtain\n",
    "        an initial near-minimal set of batches, then split some of the larger\n",
    "        batches (never increases cost) until\n",
    "\n",
    "            * every worker has at least one batch,\n",
    "            * the total number of batches is a multiple of ``workers``.\n",
    "\n",
    "        Because each split only lowers the cost of the two resulting batches,\n",
    "        the constraint in (1) remains satisfied throughout.\n",
    "    \"\"\"\n",
    "\n",
    "    if workers is None:\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    else:\n",
    "        world_size = workers\n",
    "\n",
    "    if not doc_lengths:\n",
    "        raise RuntimeError(\"Empty document list.\")\n",
    "    if max(doc_lengths) > N:  # a single document would overflow any batch\n",
    "        raise RuntimeError(\"At least one document is too long for the budget N.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) First-fit decreasing (FFD) bin packing under the cost function\n",
    "    #    cost(batch) = max_len_in_batch * len(batch)\n",
    "    # ---------------------------------------------------------------------\n",
    "    docs_sorted = sorted(enumerate(doc_lengths), key=lambda x: x[1], reverse=True)\n",
    "    batches: list[list[int]] = []  # holds document *indices*\n",
    "    batch_meta = []  # (max_len, size) for each batch\n",
    "\n",
    "    for idx, length in docs_sorted:\n",
    "        placed = False\n",
    "        for j, (mx, sz) in enumerate(batch_meta):\n",
    "            new_mx = max(mx, length)\n",
    "            new_sz = sz + 1\n",
    "            if new_mx * new_sz <= N:  # still fits\n",
    "                batches[j].append(idx)\n",
    "                batch_meta[j] = (new_mx, new_sz)\n",
    "                placed = True\n",
    "                break\n",
    "\n",
    "        if not placed:  # open a new batch\n",
    "            batches.append([idx])\n",
    "            batch_meta.append((length, 1))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Ensure every worker gets ≥ 1 batch\n",
    "    # ---------------------------------------------------------------------\n",
    "    if len(batches) < world_size:\n",
    "        # split the largest batches (by size) until we have ≥ workers batches\n",
    "        batches.sort(key=len, reverse=True)\n",
    "        while len(batches) < world_size:\n",
    "            big = batches.pop(0)  # take the current largest\n",
    "            if len(big) == 1:  # cannot split a singleton\n",
    "                raise RuntimeError(\"Not enough documents to give each worker at least one batch.\")\n",
    "            batches.append([big.pop()])  # move one doc into new batch\n",
    "            batches.append(big)  # put the remainder back\n",
    "            # preserve cost constraint automatically\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) Pad the number of batches to a multiple of `workers`\n",
    "    # ---------------------------------------------------------------------\n",
    "    k = -(-len(batches) // world_size)  # ceiling division\n",
    "    target_batches = world_size * k  # == k batches per worker\n",
    "\n",
    "    # Split arbitrary (non-singleton) batches until we reach the target\n",
    "    i = 0\n",
    "    while len(batches) < target_batches:\n",
    "        batch = batches[i % len(batches)]\n",
    "        if len(batch) == 1:\n",
    "            i += 1  # try another batch\n",
    "            continue\n",
    "        batches.append([batch.pop()])  # split off a singleton\n",
    "        i += 1\n",
    "\n",
    "    assert len(batches) == target_batches\n",
    "    assert all(max(doc_lengths[i] for i in batch) * len(batch) <= N for batch in batches)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) Round-robin assignment to workers\n",
    "    # ---------------------------------------------------------------------\n",
    "    allocation: list[list[list[int]]] = [[] for _ in range(world_size)]\n",
    "    for b_idx, batch in enumerate(batches):\n",
    "        allocation[b_idx % world_size].append(batch)\n",
    "\n",
    "    # sanity: equal # of batches per worker\n",
    "    assert len({len(b) for b in allocation}) == 1\n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "# Set all random seeds\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_all_seeds(42)  # or whatever seed you prefer\n",
    "\n",
    "# Force deterministic behavior (sacrifices speed for reproducibility)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Set environment variables for additional determinism\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.join(current_path, \"test_files\", \"pile_100_examples\")\n",
    "\n",
    "test_path = parent_path + \"/ground_truth\"\n",
    "ekfac_run_path = parent_path + \"/run/influence_results\"\n",
    "\n",
    "\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "cfg = IndexConfig(run_path=\"\")  # empty run path because we are not using it to save data\n",
    "cfg.model = \"EleutherAI/Pythia-14m\"\n",
    "cfg.precision = \"fp32\"\n",
    "cfg.fsdp = False\n",
    "\n",
    "\n",
    "cfg.data = DataConfig(dataset=parent_path + \"/data\")\n",
    "# cfg.data = DataConfig(dataset=\"NeelNanda/pile-10k\")\n",
    "\n",
    "data_str = cfg.data.dataset\n",
    "\n",
    "# save cfg\n",
    "with open(os.path.join(test_path, \"index_config.json\"), \"w\") as f:\n",
    "    json.dump(asdict(cfg), f, indent=4)\n",
    "\n",
    "\n",
    "workers = 8  # simulating n workers, but we will run on a single GPU to get ground truth\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "match cfg.precision:\n",
    "    case \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    case \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    case \"fp32\":\n",
    "        dtype = torch.float32\n",
    "    case \"int4\" | \"int8\":\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    case other:\n",
    "        raise ValueError(f\"Unsupported precision: {other}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_name = \"layers.0.mlp.dense_h_to_4h\"  # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=(\n",
    "        BitsAndBytesConfig(\n",
    "            load_in_4bit=cfg.precision == \"int4\",\n",
    "            load_in_8bit=cfg.precision == \"int8\",\n",
    "            bnb_4bit_compute_dtype=dtype,\n",
    "            bnb_4bit_quant_storage=dtype,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        if cfg.precision in (\"int4\", \"int8\")\n",
    "        else None\n",
    "    ),\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = cfg.data.dataset\n",
    "if data_str.endswith(\".csv\"):\n",
    "    ds = assert_type(Dataset, Dataset.from_csv(data_str))\n",
    "elif data_str.endswith(\".json\") or data_str.endswith(\".jsonl\"):\n",
    "    ds = assert_type(Dataset, Dataset.from_json(data_str))\n",
    "else:\n",
    "    try:\n",
    "        ds = load_dataset(data_str, split=\"train\")\n",
    "\n",
    "        if isinstance(ds, DatasetDict) or isinstance(ds, IterableDatasetDict):\n",
    "            raise NotImplementedError(\"DatasetDicts and IterableDatasetDicts are not supported.\")\n",
    "    except ValueError as e:\n",
    "        # Automatically use load_from_disk if appropriate\n",
    "        if \"load_from_disk\" in str(e):\n",
    "            ds = Dataset.load_from_disk(data_str, keep_in_memory=False)\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ds, Dataset)  # pleasing the typechecker\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model, model_max_length=cfg.token_batch_size)\n",
    "\n",
    "\n",
    "ds = ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs=dict(args=cfg.data, tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "data = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_world = allocate_batches_test(doc_lengths=ds[\"length\"], N=cfg.token_batch_size, workers=workers)\n",
    "assert len(batches_world) == workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = None\n",
    "normalizers = {}\n",
    "\n",
    "processor = GradientProcessor(\n",
    "    projection_dim=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute activation and gradient covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_test_path = os.path.join(test_path, \"covariances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(rank: int, activation_covariances={}, gradient_covariances={}):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    def callback_activation(name: str, a: torch.Tensor):\n",
    "        activation_covariance = activation_covariances.get(name, None)  # Our stored slice\n",
    "\n",
    "        a = a.reshape(-1, a.shape[-1])  # [N*S, O]\n",
    "\n",
    "        # if name == debug_name:\n",
    "        #     print(a[0, 0])\n",
    "        update = a.mT @ a\n",
    "        # if name == debug_name:\n",
    "        #     print(update[0, 0])\n",
    "\n",
    "        if activation_covariance is None:\n",
    "            activation_covariances[name] = update\n",
    "\n",
    "        else:\n",
    "            # Add it to our permanently stored slice\n",
    "            activation_covariance.add_(update)\n",
    "\n",
    "    def callback_gradient(name: str, g: torch.Tensor):\n",
    "        gradient_covariance = gradient_covariances.get(name, None)\n",
    "\n",
    "        g = g.reshape(-1, g.shape[-1])  # [N*S, O]\n",
    "\n",
    "        # if name == debug_name:\n",
    "        #     print(g.abs().sum(), rank)\n",
    "        update = g.mT @ g\n",
    "\n",
    "        if gradient_covariance is None:\n",
    "            gradient_covariances[name] = update\n",
    "        else:\n",
    "            gradient_covariance.add_(update)\n",
    "\n",
    "    # def callback_gradient(name: str, g: torch.Tensor):\n",
    "    #     gradient_covariance = gradient_covariances.get(name, None)\n",
    "\n",
    "    #     g = g.reshape(-1, g.shape[-1])  # [N*S, O]\n",
    "    #     # if name == debug_name:\n",
    "    #     #     print(g.abs().sum(), rank)\n",
    "\n",
    "    #     if gradient_covariance is None:\n",
    "    #         gradient_covariances[name] = g.sum(dim=0)\n",
    "    #     else:\n",
    "    #         gradient_covariance.add_(g.sum(dim=0))\n",
    "\n",
    "    collector = EkfacCollector(\n",
    "        model.base_model,\n",
    "        closure=callback_gradient,\n",
    "        target_modules=target_modules,\n",
    "        fwd_closure=callback_activation,\n",
    "    )\n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            loss_list.append(losses.detach().cpu())\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f846066dadf64e259e1dc6bf97ab707d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 processed 14880 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "916ce4c0f30c4262a96d20636c5cb99e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 processed 16109 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6dfca9ef8bd419aa4253ed4f8353d9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2 processed 11827 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3a2b3af85a349c0ae984ae6eac94683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3 processed 11518 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae00baa99e4478bbedf109a7eaa221c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 4 processed 10873 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb46b03bbeb44bb298424bbd54ff3569",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 processed 15216 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042a213242fa4578865af966c65ece50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 6 processed 9965 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1499441c4264000a90e20d3e3127830",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 7 processed 11038 tokens.\n"
     ]
    }
   ],
   "source": [
    "total_processed_global = 0\n",
    "for rank in range(workers):\n",
    "    covariance_test_path_rank = os.path.join(covariance_test_path, f\"rank_{rank}\")\n",
    "    os.makedirs(covariance_test_path_rank, exist_ok=True)\n",
    "\n",
    "    activation_covariances = {}\n",
    "    gradient_covariances = {}\n",
    "    d = compute_covariance(\n",
    "        rank=rank, activation_covariances=activation_covariances, gradient_covariances=gradient_covariances\n",
    "    )\n",
    "\n",
    "    save_file(activation_covariances, os.path.join(covariance_test_path_rank, \"activation_covariance.safetensors\"))\n",
    "    save_file(gradient_covariances, os.path.join(covariance_test_path_rank, \"gradient_covariance.safetensors\"))\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"w\") as f:\n",
    "        json.dump({\"total_processed_rank\": d[\"total_processed_rank\"]}, f, indent=4)\n",
    "        print(f\"Rank {rank} processed {d['total_processed_rank']} tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./test_files/pile_100_examples\"\n",
    "covariance_type = \"activation\"\n",
    "run_path = os.path.join(test_dir, \"run/influence_results\")\n",
    "\n",
    "ground_truth_path = os.path.join(test_dir, \"ground_truth\")\n",
    "covariances_ground_truth_path = os.path.join(ground_truth_path, f\"covariances/{covariance_type}_covariance.safetensors\")\n",
    "covariances_run_path = os.path.join(run_path, f\"{covariance_type}_covariance_sharded\")\n",
    "\n",
    "# load ground_truth\n",
    "ground_truth_covariances = TensorDict(load_file(covariances_ground_truth_path))\n",
    "\n",
    "world_size = len(os.listdir(covariances_run_path))  # number of shards\n",
    "# load run covariances shards and concatenate them\n",
    "\n",
    "run_covariances_shards = [os.path.join(covariances_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)]\n",
    "run_covariances_list = [(load_file(shard)) for shard in run_covariances_shards]\n",
    "run_covariances = {}\n",
    "for k, v in run_covariances_list[0].items():\n",
    "    run_covariances[k] = torch.cat([shard[k] for shard in run_covariances_list], dim=0)\n",
    "\n",
    "run_covariances = TensorDict(run_covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from all ranks\n",
    "activation_covariances = TensorDict({})\n",
    "gradient_covariances = TensorDict({})\n",
    "total_processed_global = 0\n",
    "loss_list = []\n",
    "\n",
    "for rank in range(workers):\n",
    "    covariance_test_path_rank = os.path.join(covariance_test_path, f\"rank_{rank}\")\n",
    "\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        total_processed_global += d[\"total_processed_rank\"]\n",
    "\n",
    "    # TensorDict wrapper to simplify tensor operations over dicts of tensors\n",
    "    activation_covariances_rank = TensorDict(\n",
    "        load_file(os.path.join(covariance_test_path_rank, \"activation_covariance.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    gradient_covariances_rank = TensorDict(\n",
    "        load_file(os.path.join(covariance_test_path_rank, \"gradient_covariance.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    if not activation_covariances:\n",
    "        activation_covariances = activation_covariances_rank\n",
    "    else:\n",
    "        activation_covariances = activation_covariances + activation_covariances_rank\n",
    "\n",
    "    if not gradient_covariances:\n",
    "        gradient_covariances = gradient_covariances_rank\n",
    "    else:\n",
    "        gradient_covariances = gradient_covariances + (gradient_covariances_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global processed 101426 tokens.\n"
     ]
    }
   ],
   "source": [
    "save_file(activation_covariances.to_dict(), os.path.join(covariance_test_path, \"activation_covariance.safetensors\"))\n",
    "save_file(gradient_covariances.to_dict(), os.path.join(covariance_test_path, \"gradient_covariance.safetensors\"))\n",
    "with open(os.path.join(covariance_test_path, \"stats.json\"), \"w\") as f:\n",
    "    json.dump({\"total_processed_global\": total_processed_global}, f, indent=4)\n",
    "    print(f\"Global processed {total_processed_global} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, eigh will be done in float64!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors_test_path = os.path.join(test_path, \"eigenvectors\")\n",
    "os.makedirs(eigenvectors_test_path, exist_ok=True)\n",
    "\n",
    "eigenvectors_activations = {}\n",
    "eigenvectors_gradients = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load activation and gradient covariance, and total processed\n",
    "with open(os.path.join(covariance_test_path, \"stats.json\"), \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    total_processed_global = d[\"total_processed_global\"]\n",
    "\n",
    "activation_covariances = load_file(os.path.join(covariance_test_path, \"activation_covariance.safetensors\"))\n",
    "gradient_covariances = load_file(os.path.join(covariance_test_path, \"gradient_covariance.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use run_path to see if everything is correct from 3. and errors don't propagate\n",
    "\n",
    "# covariance_a_run_path = os.path.join(ekfac_run_path, \"activation_covariance_sharded/shard_0.safetensors\")\n",
    "# covariance_g_run_path = os.path.join(ekfac_run_path, \"gradient_covariance_sharded/shard_0.safetensors\")\n",
    "# activation_covariances = load_file(covariance_a_run_path)\n",
    "# gradient_covariances = load_file(covariance_g_run_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.attention.dense tensor(-9.7376, device='cuda:0', dtype=torch.float64) tensor(11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.0.attention.query_key_value tensor(10.8752, device='cuda:0', dtype=torch.float64) tensor(-1.1103, device='cuda:0', dtype=torch.float64)\n",
      "layers.0.mlp.dense_4h_to_h tensor(-0.7598, device='cuda:0', dtype=torch.float64) tensor(11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.0.mlp.dense_h_to_4h tensor(-11.6196, device='cuda:0', dtype=torch.float64) tensor(-27.7218, device='cuda:0', dtype=torch.float64)\n",
      "layers.1.attention.dense tensor(-3.0154, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.1.attention.query_key_value tensor(-11.4285, device='cuda:0', dtype=torch.float64) tensor(-18.3982, device='cuda:0', dtype=torch.float64)\n",
      "layers.1.mlp.dense_4h_to_h tensor(24.9510, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.1.mlp.dense_h_to_4h tensor(13.0856, device='cuda:0', dtype=torch.float64) tensor(47.3072, device='cuda:0', dtype=torch.float64)\n",
      "layers.2.attention.dense tensor(5.3208, device='cuda:0', dtype=torch.float64) tensor(11.3136, device='cuda:0', dtype=torch.float64)\n",
      "layers.2.attention.query_key_value tensor(10.7620, device='cuda:0', dtype=torch.float64) tensor(-23.2947, device='cuda:0', dtype=torch.float64)\n",
      "layers.2.mlp.dense_4h_to_h tensor(5.2933, device='cuda:0', dtype=torch.float64) tensor(11.3136, device='cuda:0', dtype=torch.float64)\n",
      "layers.2.mlp.dense_h_to_4h tensor(12.3074, device='cuda:0', dtype=torch.float64) tensor(-22.8185, device='cuda:0', dtype=torch.float64)\n",
      "layers.3.attention.dense tensor(-6.5829, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.3.attention.query_key_value tensor(-14.9237, device='cuda:0', dtype=torch.float64) tensor(-12.8593, device='cuda:0', dtype=torch.float64)\n",
      "layers.3.mlp.dense_4h_to_h tensor(17.0578, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.3.mlp.dense_h_to_4h tensor(-13.9615, device='cuda:0', dtype=torch.float64) tensor(1.7990, device='cuda:0', dtype=torch.float64)\n",
      "layers.4.attention.dense tensor(36.8482, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.4.attention.query_key_value tensor(-15.0937, device='cuda:0', dtype=torch.float64) tensor(-28.4968, device='cuda:0', dtype=torch.float64)\n",
      "layers.4.mlp.dense_4h_to_h tensor(-10.4965, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.4.mlp.dense_h_to_4h tensor(-11.2878, device='cuda:0', dtype=torch.float64) tensor(2.0551, device='cuda:0', dtype=torch.float64)\n",
      "layers.5.attention.dense tensor(6.3652, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.5.attention.query_key_value tensor(-6.6954, device='cuda:0', dtype=torch.float64) tensor(-14.4253, device='cuda:0', dtype=torch.float64)\n",
      "layers.5.mlp.dense_4h_to_h tensor(21.8138, device='cuda:0', dtype=torch.float64) tensor(-11.3137, device='cuda:0', dtype=torch.float64)\n",
      "layers.5.mlp.dense_h_to_4h tensor(-12.8444, device='cuda:0', dtype=torch.float64) tensor(-20.1133, device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for name in activation_covariances.keys():\n",
    "    a = activation_covariances[name].to(dtype=torch.float64, device=device)\n",
    "    g = gradient_covariances[name].to(dtype=torch.float64, device=device)\n",
    "    a = (a + a.T).div(2)\n",
    "    g = (g + g.T).div(2)\n",
    "    a.div_(total_processed_global)\n",
    "    g.div_(total_processed_global)\n",
    "\n",
    "    eigenvalues_a, eigenvectors_a = torch.linalg.eigh(a)\n",
    "    eigenvalues_g, eigenvectors_g = torch.linalg.eigh(g)\n",
    "    print(name, eigenvectors_a.sum(), eigenvectors_g.sum())\n",
    "    eigenvectors_activations[name] = eigenvectors_a.to(dtype=dtype).contiguous()\n",
    "    eigenvectors_gradients[name] = eigenvectors_g.to(dtype=dtype).contiguous()\n",
    "\n",
    "save_file(eigenvectors_activations, os.path.join(eigenvectors_test_path, \"eigenvectors_activations.safetensors\"))\n",
    "save_file(eigenvectors_gradients, os.path.join(eigenvectors_test_path, \"eigenvectors_gradients.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute eigenvalue correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_correction_test_path = os.path.join(test_path, \"eigenvalue_corrections\")\n",
    "os.makedirs(eigenvalue_correction_test_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eigenvectors\n",
    "eigenvectors_activations = load_file(os.path.join(eigenvectors_test_path, \"eigenvectors_activations.safetensors\"))\n",
    "eigenvectors_gradients = load_file(os.path.join(eigenvectors_test_path, \"eigenvectors_gradients.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load eigenvectors from run\n",
    "# eigenvectors_activations_run_path = os.path.join(ekfac_run_path, \"activation_eigen_sharded\")\n",
    "\n",
    "\n",
    "# world_size = len(os.listdir(eigenvectors_activations_run_path))  # number of shards\n",
    "# # load run eigenvectors shards and concatenate them\n",
    "# run_eigenvectors_shards = [\n",
    "#     os.path.join(eigenvectors_activations_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)\n",
    "# ]\n",
    "# run_eigenvectors_list = [(load_file(shard)) for shard in run_eigenvectors_shards]\n",
    "# run_eigenvectors = {}\n",
    "# for k, v in run_eigenvectors_list[0].items():\n",
    "#     run_eigenvectors[k] = torch.cat([shard[k] for shard in run_eigenvectors_list], dim=0)\n",
    "\n",
    "# eigenvectors_activations = TensorDict(run_eigenvectors)\n",
    "\n",
    "\n",
    "# eigenvectors_gradients_run_path = os.path.join(ekfac_run_path, \"gradient_eigen_sharded\")\n",
    "\n",
    "\n",
    "# world_size = len(os.listdir(eigenvectors_gradients_run_path))  # number of shards\n",
    "# # load run eigenvectors shards and concatenate them\n",
    "# run_eigenvectors_shards = [\n",
    "#     os.path.join(eigenvectors_gradients_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)\n",
    "# ]\n",
    "# run_eigenvectors_list = [(load_file(shard)) for shard in run_eigenvectors_shards]\n",
    "# run_eigenvectors = {}\n",
    "# for k, v in run_eigenvectors_list[0].items():\n",
    "#     run_eigenvectors[k] = torch.cat([shard[k] for shard in run_eigenvectors_list], dim=0)\n",
    "\n",
    "# eigenvectors_gradients = TensorDict(run_eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_corrections = {}\n",
    "activation_cache = {}\n",
    "\n",
    "\n",
    "# only for debugging\n",
    "gradient_cache = {}\n",
    "pseudo_grad_cache = {}\n",
    "transformed_activation_cache = {}\n",
    "key_debug = \"layers.0.mlp.dense_h_to_4h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalue_correction(\n",
    "    rank: int,\n",
    "    eigenvalue_corrections,\n",
    "    eigenvectors_activations=eigenvectors_activations,\n",
    "    eigenvectors_gradients=eigenvectors_gradients,\n",
    "):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    def callback_activation(name: str, a: torch.Tensor):\n",
    "        # a = torch.ones_like(a)  # for debugging, pretend all activations are 1\n",
    "        activation = a  # [N, S, I]\n",
    "        activation_cache[name] = activation\n",
    "\n",
    "    def callback_gradient(name: str, g: torch.Tensor):\n",
    "        eigenvector_a = eigenvectors_activations[name].to(device=device)\n",
    "        eigenvector_g = eigenvectors_gradients[name].to(device=device)\n",
    "\n",
    "        gradient = g  # [N, S, O]\n",
    "\n",
    "        if name == debug_name:\n",
    "            gradient_cache[name] = g\n",
    "        gradient = torch.einsum(\"N S O, N S I -> N S O I\", g, activation_cache[name])\n",
    "\n",
    "        gradient = torch.einsum(\"N S O I, I J -> N S O J\", gradient, eigenvector_a)\n",
    "        gradient = torch.einsum(\" O P, N S O J -> N S P J\", eigenvector_g, gradient)\n",
    "        gradient = gradient.sum(dim=1)  # sum over sequence length\n",
    "\n",
    "        gradient = gradient**2\n",
    "        correction = gradient.sum(dim=0)\n",
    "\n",
    "        if name not in eigenvalue_corrections:\n",
    "            eigenvalue_corrections[name] = correction\n",
    "        else:\n",
    "            eigenvalue_corrections[name].add_(correction)\n",
    "\n",
    "    collector = EkfacCollector(\n",
    "        model.base_model,\n",
    "        closure=callback_gradient,\n",
    "        target_modules=target_modules,\n",
    "        fwd_closure=callback_activation,\n",
    "    )\n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_cache_amortized = {}\n",
    "\n",
    "\n",
    "def compute_eigenvalue_correction_amortized(\n",
    "    rank: int,\n",
    "    eigenvalue_corrections,\n",
    "    eigenvectors_activations=eigenvectors_activations,\n",
    "    eigenvectors_gradients=eigenvectors_gradients,\n",
    "):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    def callback_activation(name: str, a: torch.Tensor):\n",
    "        # a = torch.ones_like(a)  # for debugging, pretend all activations are 1\n",
    "        activation = a  # [N, S, I]\n",
    "        activation_cache[name] = activation\n",
    "\n",
    "    def callback_gradient(name: str, g: torch.Tensor):\n",
    "        eigenvector_a = eigenvectors_activations[name].to(device=device)\n",
    "        eigenvector_g = eigenvectors_gradients[name].to(device=device)\n",
    "        gradient = g  # [N, S, O]\n",
    "\n",
    "        transformed_a = torch.einsum(\"N S I, I J -> N S J\", activation_cache[name], eigenvector_a)\n",
    "        transformed_g = torch.einsum(\"O P, N S O -> N S P\", eigenvector_g, gradient)\n",
    "        correction = (torch.einsum(\" N S O, N S I ->N O I\", transformed_g, transformed_a) ** 2).sum(dim=0).contiguous()\n",
    "        # torch.save(activation_cache[name], \"activation.pt\")\n",
    "        # torch.save(transformed_a, \"transformed_a.pt\")\n",
    "        # torch.save(transformed_g, \"transformed_g.pt\")\n",
    "        # torch.save(correction, \"correction.pt\")\n",
    "\n",
    "        if name == debug_name:\n",
    "            gradient_cache_amortized[name] = g\n",
    "        if name not in eigenvalue_corrections:\n",
    "            eigenvalue_corrections[name] = correction\n",
    "        else:\n",
    "            eigenvalue_corrections[name].add_(correction)\n",
    "\n",
    "    collector = EkfacCollector(\n",
    "        model.base_model,\n",
    "        closure=callback_gradient,\n",
    "        target_modules=target_modules,\n",
    "        fwd_closure=callback_activation,\n",
    "    )\n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0beeae5588d4f5e92fce5ba10687a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 0 processed 14880 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc0fa335d449d1b859354c8084a954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 processed 16109 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a3333bfc5b43e68026e3cc1402d269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 2 processed 11827 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66eaa239ae504b1685017e211ef892e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 3 processed 11518 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c634115ba594fa29a097337f66c1097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 4 processed 10873 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "873152b1acc042588e40816a34413cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 5 processed 15216 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58fc9e093474306bd15eb5bb80bf81d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 6 processed 9965 tokens.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "522832824260449382a79bf861d44c22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 7 processed 11038 tokens.\n"
     ]
    }
   ],
   "source": [
    "eigenvalue_corrections = {}\n",
    "\n",
    "total_processed_global = 0\n",
    "for rank in range(workers):\n",
    "    eigenvalue_correction_test_path_rank = os.path.join(eigenvalue_correction_test_path, f\"rank_{rank}\")\n",
    "    os.makedirs(eigenvalue_correction_test_path_rank, exist_ok=True)\n",
    "\n",
    "    eigenvalue_corrections = {}\n",
    "    d = compute_eigenvalue_correction_amortized(\n",
    "        rank=rank,\n",
    "        eigenvalue_corrections=eigenvalue_corrections,\n",
    "        eigenvectors_activations=eigenvectors_activations,\n",
    "        eigenvectors_gradients=eigenvectors_gradients,\n",
    "    )\n",
    "\n",
    "    # d = compute_eigenvalue_correction(\n",
    "    #     rank=rank,\n",
    "    #     eigenvalue_corrections=eigenvalue_corrections,\n",
    "    #     eigenvectors_activations=eigenvectors_activations,\n",
    "    #     eigenvectors_gradients=eigenvectors_gradients,\n",
    "    # )\n",
    "\n",
    "    save_file(\n",
    "        eigenvalue_corrections, os.path.join(eigenvalue_correction_test_path_rank, \"eigenvalue_corrections.safetensors\")\n",
    "    )\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"w\") as f:\n",
    "        json.dump({\"total_processed_rank\": d[\"total_processed_rank\"]}, f, indent=4)\n",
    "        print(f\"Rank {rank} processed {d['total_processed_rank']} tokens.\")\n",
    "    total_processed_global += d[\"total_processed_rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from all ranks\n",
    "eigenvalue_corrections = TensorDict({})\n",
    "\n",
    "\n",
    "for rank in range(workers):\n",
    "    eigenvalue_correction_test_path_rank = os.path.join(eigenvalue_correction_test_path, f\"rank_{rank}\")\n",
    "\n",
    "    # TensorDict wrapper to simplify tensor operations over dicts of tensors\n",
    "    eigenvalue_corrections_rank = TensorDict(\n",
    "        load_file(os.path.join(eigenvalue_correction_test_path_rank, \"eigenvalue_corrections.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    if not eigenvalue_corrections:\n",
    "        eigenvalue_corrections = eigenvalue_corrections_rank\n",
    "    else:\n",
    "        eigenvalue_corrections = eigenvalue_corrections + (eigenvalue_corrections_rank)\n",
    "\n",
    "eigenvalue_corrections.div_(total_processed_global)\n",
    "save_file(\n",
    "    eigenvalue_corrections.to_dict(),\n",
    "    os.path.join(eigenvalue_correction_test_path, \"eigenvalue_corrections.safetensors\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d_length = 10\n",
    "test_activations = {}\n",
    "test_gradients = {}\n",
    "batch_sizes = torch.randint(4000, 8000, (d_length,))\n",
    "activation_length = torch.randint(3000, 5000, (d_length,))\n",
    "gradient_length = torch.randint(3000, 10000, (d_length,))\n",
    "# Create test activations\n",
    "for i in range(d_length):\n",
    "    test_activations[f\"layer_{i}\"] = torch.rand(batch_sizes[i].item(), activation_length[i].item())\n",
    "    test_gradients[f\"layer_{i}\"] = torch.rand(batch_sizes[i].item(), gradient_length[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict({'layer_0': torch.Size([5542, 4378]), 'layer_1': torch.Size([6067, 4014]), 'layer_2': torch.Size([6876, 4210]), 'layer_3': torch.Size([6414, 3954]), 'layer_4': torch.Size([6026, 3231]), 'layer_5': torch.Size([7335, 4572]), 'layer_6': torch.Size([6620, 4315]), 'layer_7': torch.Size([7924, 3295]), 'layer_8': torch.Size([6950, 3567]), 'layer_9': torch.Size([5113, 3706])})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TensorDict(test_activations).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "n, s, b = 10, 23, 30\n",
    "x = torch.empty((n, s, b), device=device, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
