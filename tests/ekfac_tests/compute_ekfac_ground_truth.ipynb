{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import os\n",
    "from dataclasses import asdict\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.nn.functional as F\n",
    "from datasets import Dataset, DatasetDict, IterableDatasetDict, load_dataset\n",
    "from safetensors.torch import load_file, save_file\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "from bergson.data import DataConfig, IndexConfig, pad_and_tensor, tokenize\n",
    "from bergson.gradients import (\n",
    "    GradientProcessor,\n",
    ")\n",
    "from bergson.hessians.collector import CovarianceCollector, HookCollectorBase, LambdaCollector\n",
    "from bergson.hessians.utils import TensorDict\n",
    "from bergson.utils import assert_type\n",
    "from ground_truth.collector import (\n",
    "    GroundTruthAmortizedLambdaCollector,\n",
    "    GroundTruthCovarianceCollector,\n",
    "    GroundTruthNonAmortizedLambdaCollector,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -1. Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_batches_test(doc_lengths: list[int], N: int, workers: Optional[int] = None) -> list[list[list[int]]]:\n",
    "    \"\"\"\n",
    "    Modification of allocate_batches to return a flat list of batches for testing(instead of returning allocation[rank])\n",
    "    Allocate documents into batches that are then distributed evenly across\n",
    "    a fixed number of workers.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lengths : Sequence[int]\n",
    "        Length (in tokens) of each document.  The *i-th* document is referred to\n",
    "        internally by its index ``i``.\n",
    "    workers : int\n",
    "        Number of parallel workers ( 1 ≤ workers ≤ 8).\n",
    "    N : int\n",
    "        Hard memory budget per *batch*, expressed as\n",
    "        ``max(length in batch) * (# docs in batch) ≤ N``.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list[list[list[int]]]\n",
    "        ``allocation[w][b]`` is the list of document indices that belong to the\n",
    "        *b-th* batch assigned to worker ``w``.  Every worker receives the same\n",
    "        number of (non-empty) batches.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    AllocationError\n",
    "        If the three hard constraints cannot be satisfied.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    1.  **Per-batch cost constraint**:  Each batch is padded to the maximum\n",
    "        sequence length *inside that batch*, so its cost in “token × examples”\n",
    "        units is ``max_len_in_batch * batch_size``.  This must stay ≤ ``N``.\n",
    "    2.  **Bin-packing strategy**:  We use *first-fit decreasing* (FFD) to obtain\n",
    "        an initial near-minimal set of batches, then split some of the larger\n",
    "        batches (never increases cost) until\n",
    "\n",
    "            * every worker has at least one batch,\n",
    "            * the total number of batches is a multiple of ``workers``.\n",
    "\n",
    "        Because each split only lowers the cost of the two resulting batches,\n",
    "        the constraint in (1) remains satisfied throughout.\n",
    "    \"\"\"\n",
    "\n",
    "    if workers is None:\n",
    "        world_size = dist.get_world_size() if dist.is_initialized() else 1\n",
    "    else:\n",
    "        world_size = workers\n",
    "\n",
    "    if not doc_lengths:\n",
    "        raise RuntimeError(\"Empty document list.\")\n",
    "    if max(doc_lengths) > N:  # a single document would overflow any batch\n",
    "        raise RuntimeError(\"At least one document is too long for the budget N.\")\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 1) First-fit decreasing (FFD) bin packing under the cost function\n",
    "    #    cost(batch) = max_len_in_batch * len(batch)\n",
    "    # ---------------------------------------------------------------------\n",
    "    docs_sorted = sorted(enumerate(doc_lengths), key=lambda x: x[1], reverse=True)\n",
    "    batches: list[list[int]] = []  # holds document *indices*\n",
    "    batch_meta = []  # (max_len, size) for each batch\n",
    "\n",
    "    for idx, length in docs_sorted:\n",
    "        placed = False\n",
    "        for j, (mx, sz) in enumerate(batch_meta):\n",
    "            new_mx = max(mx, length)\n",
    "            new_sz = sz + 1\n",
    "            if new_mx * new_sz <= N:  # still fits\n",
    "                batches[j].append(idx)\n",
    "                batch_meta[j] = (new_mx, new_sz)\n",
    "                placed = True\n",
    "                break\n",
    "\n",
    "        if not placed:  # open a new batch\n",
    "            batches.append([idx])\n",
    "            batch_meta.append((length, 1))\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 2) Ensure every worker gets ≥ 1 batch\n",
    "    # ---------------------------------------------------------------------\n",
    "    if len(batches) < world_size:\n",
    "        # split the largest batches (by size) until we have ≥ workers batches\n",
    "        batches.sort(key=len, reverse=True)\n",
    "        while len(batches) < world_size:\n",
    "            big = batches.pop(0)  # take the current largest\n",
    "            if len(big) == 1:  # cannot split a singleton\n",
    "                raise RuntimeError(\"Not enough documents to give each worker at least one batch.\")\n",
    "            batches.append([big.pop()])  # move one doc into new batch\n",
    "            batches.append(big)  # put the remainder back\n",
    "            # preserve cost constraint automatically\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 3) Pad the number of batches to a multiple of `workers`\n",
    "    # ---------------------------------------------------------------------\n",
    "    k = -(-len(batches) // world_size)  # ceiling division\n",
    "    target_batches = world_size * k  # == k batches per worker\n",
    "\n",
    "    # Split arbitrary (non-singleton) batches until we reach the target\n",
    "    i = 0\n",
    "    while len(batches) < target_batches:\n",
    "        batch = batches[i % len(batches)]\n",
    "        if len(batch) == 1:\n",
    "            i += 1  # try another batch\n",
    "            continue\n",
    "        batches.append([batch.pop()])  # split off a singleton\n",
    "        i += 1\n",
    "\n",
    "    assert len(batches) == target_batches\n",
    "    assert all(max(doc_lengths[i] for i in batch) * len(batch) <= N for batch in batches)\n",
    "\n",
    "    # ---------------------------------------------------------------------\n",
    "    # 4) Round-robin assignment to workers\n",
    "    # ---------------------------------------------------------------------\n",
    "    allocation: list[list[list[int]]] = [[] for _ in range(world_size)]\n",
    "    for b_idx, batch in enumerate(batches):\n",
    "        allocation[b_idx % world_size].append(batch)\n",
    "\n",
    "    # sanity: equal # of batches per worker\n",
    "    assert len({len(b) for b in allocation}) == 1\n",
    "    return allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.backends import cudnn\n",
    "\n",
    "\n",
    "# Set all random seeds\n",
    "def set_all_seeds(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # for multi-GPU\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "set_all_seeds(42)  # or whatever seed you prefer\n",
    "\n",
    "# Force deterministic behavior (sacrifices speed for reproducibility)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "# Set environment variables for additional determinism\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_path = os.getcwd()\n",
    "parent_path = os.path.join(current_path, \"test_files\", \"pile_100_examples\")\n",
    "\n",
    "test_path = parent_path + \"/ground_truth\"\n",
    "ekfac_run_path = parent_path + \"/run/influence_results\"\n",
    "\n",
    "\n",
    "os.makedirs(test_path, exist_ok=True)\n",
    "cfg = IndexConfig(run_path=\"\")  # empty run path because we are not using it to save data\n",
    "cfg.model = \"EleutherAI/Pythia-14m\"\n",
    "cfg.precision = \"fp32\"\n",
    "cfg.fsdp = False\n",
    "\n",
    "\n",
    "cfg.data = DataConfig(dataset=parent_path + \"/data\")\n",
    "# cfg.data = DataConfig(dataset=\"NeelNanda/pile-10k\")\n",
    "\n",
    "data_str = cfg.data.dataset\n",
    "\n",
    "# Create pile-100 dataset if it doesn't exist\n",
    "if not os.path.exists(data_str):\n",
    "    full_dataset = load_dataset(\"NeelNanda/pile-10k\", split=\"train\")\n",
    "    subset = full_dataset.select(range(100))\n",
    "    os.makedirs(os.path.dirname(data_str), exist_ok=True)\n",
    "    subset.save_to_disk(data_str)\n",
    "    print(f\"Generated pile-100 in {data_str}\")\n",
    "\n",
    "# save cfg\n",
    "with open(os.path.join(test_path, \"index_config.json\"), \"w\") as f:\n",
    "    json.dump(asdict(cfg), f, indent=4)\n",
    "\n",
    "\n",
    "workers = 8  # simulating n workers, but we will run on a single GPU to get ground truth\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match cfg.precision:\n",
    "    case \"bf16\":\n",
    "        dtype = torch.bfloat16\n",
    "    case \"fp16\":\n",
    "        dtype = torch.float16\n",
    "    case \"fp32\":\n",
    "        dtype = torch.float32\n",
    "    case \"int4\" | \"int8\":\n",
    "        dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16\n",
    "    case other:\n",
    "        raise ValueError(f\"Unsupported precision: {other}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_name = \"layers.0.mlp.dense_h_to_4h\"  # for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading model and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    cfg.model,\n",
    "    device_map=\"cuda\",\n",
    "    quantization_config=(\n",
    "        BitsAndBytesConfig(\n",
    "            load_in_4bit=cfg.precision == \"int4\",\n",
    "            load_in_8bit=cfg.precision == \"int8\",\n",
    "            bnb_4bit_compute_dtype=dtype,\n",
    "            bnb_4bit_quant_storage=dtype,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "        )\n",
    "        if cfg.precision in (\"int4\", \"int8\")\n",
    "        else None\n",
    "    ),\n",
    "    torch_dtype=dtype,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_str = cfg.data.dataset\n",
    "if data_str.endswith(\".csv\"):\n",
    "    ds = assert_type(Dataset, Dataset.from_csv(data_str))\n",
    "elif data_str.endswith(\".json\") or data_str.endswith(\".jsonl\"):\n",
    "    ds = assert_type(Dataset, Dataset.from_json(data_str))\n",
    "else:\n",
    "    try:\n",
    "        ds = load_dataset(data_str, split=\"train\")\n",
    "\n",
    "        if isinstance(ds, DatasetDict) or isinstance(ds, IterableDatasetDict):\n",
    "            raise NotImplementedError(\"DatasetDicts and IterableDatasetDicts are not supported.\")\n",
    "    except ValueError as e:\n",
    "        # Automatically use load_from_disk if appropriate\n",
    "        if \"load_from_disk\" in str(e):\n",
    "            ds = Dataset.load_from_disk(data_str, keep_in_memory=False)\n",
    "        else:\n",
    "            raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(ds, Dataset)  # pleasing the typechecker\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(cfg.model, model_max_length=cfg.token_batch_size)\n",
    "\n",
    "\n",
    "ds = ds.map(\n",
    "    tokenize,\n",
    "    batched=True,\n",
    "    fn_kwargs=dict(args=cfg.data, tokenizer=tokenizer),\n",
    ")\n",
    "\n",
    "data = ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_world = allocate_batches_test(doc_lengths=ds[\"length\"], N=cfg.token_batch_size, workers=workers)\n",
    "assert len(batches_world) == workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_modules = None\n",
    "normalizers = {}\n",
    "\n",
    "processor = GradientProcessor(\n",
    "    projection_dim=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compute activation and gradient covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance_test_path = os.path.join(test_path, \"covariances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_covariance(rank: int, activation_covariances={}, gradient_covariances={}):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    loss_list = []\n",
    "\n",
    "    collector = GroundTruthCovarianceCollector(\n",
    "        model=model.base_model,\n",
    "        activation_covariances=activation_covariances,\n",
    "        gradient_covariances=gradient_covariances,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            loss_list.append(losses.detach().cpu())\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_processed_global = 0\n",
    "for rank in range(workers):\n",
    "    covariance_test_path_rank = os.path.join(covariance_test_path, f\"rank_{rank}\")\n",
    "    os.makedirs(covariance_test_path_rank, exist_ok=True)\n",
    "\n",
    "    activation_covariances = {}\n",
    "    gradient_covariances = {}\n",
    "    d = compute_covariance(\n",
    "        rank=rank, activation_covariances=activation_covariances, gradient_covariances=gradient_covariances\n",
    "    )\n",
    "\n",
    "    save_file(activation_covariances, os.path.join(covariance_test_path_rank, \"activation_covariance.safetensors\"))\n",
    "    save_file(gradient_covariances, os.path.join(covariance_test_path_rank, \"gradient_covariance.safetensors\"))\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"w\") as f:\n",
    "        json.dump({\"total_processed_rank\": d[\"total_processed_rank\"]}, f, indent=4)\n",
    "        print(f\"Rank {rank} processed {d['total_processed_rank']} tokens.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"./test_files/pile_100_examples\"\n",
    "covariance_type = \"activation\"\n",
    "run_path = os.path.join(test_dir, \"run/influence_results\")\n",
    "\n",
    "ground_truth_path = os.path.join(test_dir, \"ground_truth\")\n",
    "covariances_ground_truth_path = os.path.join(ground_truth_path, f\"covariances/{covariance_type}_covariance.safetensors\")\n",
    "covariances_run_path = os.path.join(run_path, f\"{covariance_type}_covariance_sharded\")\n",
    "\n",
    "# load ground_truth\n",
    "ground_truth_covariances = TensorDict(load_file(covariances_ground_truth_path))\n",
    "\n",
    "world_size = len(os.listdir(covariances_run_path))  # number of shards\n",
    "# load run covariances shards and concatenate them\n",
    "\n",
    "run_covariances_shards = [os.path.join(covariances_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)]\n",
    "run_covariances_list = [(load_file(shard)) for shard in run_covariances_shards]\n",
    "run_covariances = {}\n",
    "for k, v in run_covariances_list[0].items():\n",
    "    run_covariances[k] = torch.cat([shard[k] for shard in run_covariances_list], dim=0)\n",
    "\n",
    "run_covariances = TensorDict(run_covariances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from all ranks\n",
    "activation_covariances = TensorDict({})\n",
    "gradient_covariances = TensorDict({})\n",
    "total_processed_global = 0\n",
    "loss_list = []\n",
    "\n",
    "for rank in range(workers):\n",
    "    covariance_test_path_rank = os.path.join(covariance_test_path, f\"rank_{rank}\")\n",
    "\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"r\") as f:\n",
    "        d = json.load(f)\n",
    "        total_processed_global += d[\"total_processed_rank\"]\n",
    "\n",
    "    # TensorDict wrapper to simplify tensor operations over dicts of tensors\n",
    "    activation_covariances_rank = TensorDict(\n",
    "        load_file(os.path.join(covariance_test_path_rank, \"activation_covariance.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    gradient_covariances_rank = TensorDict(\n",
    "        load_file(os.path.join(covariance_test_path_rank, \"gradient_covariance.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    if not activation_covariances:\n",
    "        activation_covariances = activation_covariances_rank\n",
    "    else:\n",
    "        activation_covariances = activation_covariances + activation_covariances_rank\n",
    "\n",
    "    if not gradient_covariances:\n",
    "        gradient_covariances = gradient_covariances_rank\n",
    "    else:\n",
    "        gradient_covariances = gradient_covariances + (gradient_covariances_rank)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file(activation_covariances.to_dict(), os.path.join(covariance_test_path, \"activation_covariance.safetensors\"))\n",
    "save_file(gradient_covariances.to_dict(), os.path.join(covariance_test_path, \"gradient_covariance.safetensors\"))\n",
    "with open(os.path.join(covariance_test_path, \"stats.json\"), \"w\") as f:\n",
    "    json.dump({\"total_processed_global\": total_processed_global}, f, indent=4)\n",
    "    print(f\"Global processed {total_processed_global} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Compute eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, eigh will be done in float64!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvectors_test_path = os.path.join(test_path, \"eigenvectors\")\n",
    "os.makedirs(eigenvectors_test_path, exist_ok=True)\n",
    "\n",
    "eigenvectors_activations = {}\n",
    "eigenvectors_gradients = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load activation and gradient covariance, and total processed\n",
    "with open(os.path.join(covariance_test_path, \"stats.json\"), \"r\") as f:\n",
    "    d = json.load(f)\n",
    "    total_processed_global = d[\"total_processed_global\"]\n",
    "\n",
    "activation_covariances = load_file(os.path.join(covariance_test_path, \"activation_covariance.safetensors\"))\n",
    "gradient_covariances = load_file(os.path.join(covariance_test_path, \"gradient_covariance.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use run_path to see if everything is correct from 3. and errors don't propagate\n",
    "\n",
    "# covariance_a_run_path = os.path.join(ekfac_run_path, \"activation_covariance_sharded/shard_0.safetensors\")\n",
    "# covariance_g_run_path = os.path.join(ekfac_run_path, \"gradient_covariance_sharded/shard_0.safetensors\")\n",
    "# activation_covariances = load_file(covariance_a_run_path)\n",
    "# gradient_covariances = load_file(covariance_g_run_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in activation_covariances.keys():\n",
    "    a = activation_covariances[name].to(dtype=torch.float64, device=device)\n",
    "    g = gradient_covariances[name].to(dtype=torch.float64, device=device)\n",
    "    a = (a + a.T).div(2)\n",
    "    g = (g + g.T).div(2)\n",
    "    a.div_(total_processed_global)\n",
    "    g.div_(total_processed_global)\n",
    "\n",
    "    eigenvalues_a, eigenvectors_a = torch.linalg.eigh(a)\n",
    "    eigenvalues_g, eigenvectors_g = torch.linalg.eigh(g)\n",
    "    print(name, eigenvectors_a.sum(), eigenvectors_g.sum())\n",
    "    eigenvectors_activations[name] = eigenvectors_a.to(dtype=dtype).contiguous()\n",
    "    eigenvectors_gradients[name] = eigenvectors_g.to(dtype=dtype).contiguous()\n",
    "\n",
    "save_file(eigenvectors_activations, os.path.join(eigenvectors_test_path, \"eigenvectors_activations.safetensors\"))\n",
    "save_file(eigenvectors_gradients, os.path.join(eigenvectors_test_path, \"eigenvectors_gradients.safetensors\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compute eigenvalue correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_correction_test_path = os.path.join(test_path, \"eigenvalue_corrections\")\n",
    "os.makedirs(eigenvalue_correction_test_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load eigenvectors\n",
    "eigenvectors_activations = load_file(os.path.join(eigenvectors_test_path, \"eigenvectors_activations.safetensors\"))\n",
    "eigenvectors_gradients = load_file(os.path.join(eigenvectors_test_path, \"eigenvectors_gradients.safetensors\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load eigenvectors from run\n",
    "# eigenvectors_activations_run_path = os.path.join(ekfac_run_path, \"activation_eigen_sharded\")\n",
    "\n",
    "\n",
    "# world_size = len(os.listdir(eigenvectors_activations_run_path))  # number of shards\n",
    "# # load run eigenvectors shards and concatenate them\n",
    "# run_eigenvectors_shards = [\n",
    "#     os.path.join(eigenvectors_activations_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)\n",
    "# ]\n",
    "# run_eigenvectors_list = [(load_file(shard)) for shard in run_eigenvectors_shards]\n",
    "# run_eigenvectors = {}\n",
    "# for k, v in run_eigenvectors_list[0].items():\n",
    "#     run_eigenvectors[k] = torch.cat([shard[k] for shard in run_eigenvectors_list], dim=0)\n",
    "\n",
    "# eigenvectors_activations = TensorDict(run_eigenvectors)\n",
    "\n",
    "\n",
    "# eigenvectors_gradients_run_path = os.path.join(ekfac_run_path, \"gradient_eigen_sharded\")\n",
    "\n",
    "\n",
    "# world_size = len(os.listdir(eigenvectors_gradients_run_path))  # number of shards\n",
    "# # load run eigenvectors shards and concatenate them\n",
    "# run_eigenvectors_shards = [\n",
    "#     os.path.join(eigenvectors_gradients_run_path, f\"shard_{rank}.safetensors\") for rank in range(world_size)\n",
    "# ]\n",
    "# run_eigenvectors_list = [(load_file(shard)) for shard in run_eigenvectors_shards]\n",
    "# run_eigenvectors = {}\n",
    "# for k, v in run_eigenvectors_list[0].items():\n",
    "#     run_eigenvectors[k] = torch.cat([shard[k] for shard in run_eigenvectors_list], dim=0)\n",
    "\n",
    "# eigenvectors_gradients = TensorDict(run_eigenvectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_corrections = {}\n",
    "activation_cache = {}\n",
    "\n",
    "\n",
    "# only for debugging\n",
    "gradient_cache = {}\n",
    "pseudo_grad_cache = {}\n",
    "transformed_activation_cache = {}\n",
    "key_debug = \"layers.0.mlp.dense_h_to_4h\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_eigenvalue_correction(\n",
    "    rank: int,\n",
    "    eigenvalue_corrections,\n",
    "    eigenvectors_activations=eigenvectors_activations,\n",
    "    eigenvectors_gradients=eigenvectors_gradients,\n",
    "):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    # Note: This is the non-amortized version kept for reference\n",
    "    # The amortized version in cell-37 is more efficient\n",
    "    \n",
    "    collector = GroundTruthNonAmortizedLambdaCollector(\n",
    "        model=model.base_model,\n",
    "        eigenvalue_corrections=eigenvalue_corrections,\n",
    "        eigenvectors_activations=eigenvectors_activations,\n",
    "        eigenvectors_gradients=eigenvectors_gradients,\n",
    "        device=device,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_cache_amortized = {}\n",
    "\n",
    "\n",
    "def compute_eigenvalue_correction_amortized(\n",
    "    rank: int,\n",
    "    eigenvalue_corrections,\n",
    "    eigenvectors_activations=eigenvectors_activations,\n",
    "    eigenvectors_gradients=eigenvectors_gradients,\n",
    "):\n",
    "    total_processed = 0\n",
    "    batches = batches_world[rank]\n",
    "\n",
    "    collector = GroundTruthAmortizedLambdaCollector(\n",
    "        model=model.base_model,\n",
    "        eigenvalue_corrections=eigenvalue_corrections,\n",
    "        eigenvectors_activations=eigenvectors_activations,\n",
    "        eigenvectors_gradients=eigenvectors_gradients,\n",
    "        device=device,\n",
    "        target_modules=target_modules,\n",
    "    )\n",
    "    \n",
    "    for sl in tqdm(batches):\n",
    "        batch = data[sl]\n",
    "        x, y = pad_and_tensor(\n",
    "            batch[\"input_ids\"],  # type: ignore\n",
    "            labels=batch.get(\"labels\"),  # type: ignore\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        total_processed += x.numel()\n",
    "\n",
    "        with collector:\n",
    "            logits = model(x).logits\n",
    "            losses = F.cross_entropy(\n",
    "                logits[:, :-1].reshape(-1, logits.size(-1)),\n",
    "                y[:, 1:].flatten(),\n",
    "                reduction=\"none\",\n",
    "            ).reshape_as(y[:, 1:])\n",
    "\n",
    "            losses = losses.sum(1)\n",
    "\n",
    "            losses.mean().backward()\n",
    "\n",
    "            model.zero_grad()\n",
    "\n",
    "    return {\"losses\": loss_list, \"total_processed_rank\": total_processed}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalue_corrections = {}\n",
    "\n",
    "total_processed_global = 0\n",
    "for rank in range(workers):\n",
    "    eigenvalue_correction_test_path_rank = os.path.join(eigenvalue_correction_test_path, f\"rank_{rank}\")\n",
    "    os.makedirs(eigenvalue_correction_test_path_rank, exist_ok=True)\n",
    "\n",
    "    eigenvalue_corrections = {}\n",
    "    d = compute_eigenvalue_correction_amortized(\n",
    "        rank=rank,\n",
    "        eigenvalue_corrections=eigenvalue_corrections,\n",
    "        eigenvectors_activations=eigenvectors_activations,\n",
    "        eigenvectors_gradients=eigenvectors_gradients,\n",
    "    )\n",
    "\n",
    "    # d = compute_eigenvalue_correction(\n",
    "    #     rank=rank,\n",
    "    #     eigenvalue_corrections=eigenvalue_corrections,\n",
    "    #     eigenvectors_activations=eigenvectors_activations,\n",
    "    #     eigenvectors_gradients=eigenvectors_gradients,\n",
    "    # )\n",
    "\n",
    "    save_file(\n",
    "        eigenvalue_corrections, os.path.join(eigenvalue_correction_test_path_rank, \"eigenvalue_corrections.safetensors\")\n",
    "    )\n",
    "    with open(os.path.join(covariance_test_path_rank, \"stats.json\"), \"w\") as f:\n",
    "        json.dump({\"total_processed_rank\": d[\"total_processed_rank\"]}, f, indent=4)\n",
    "        print(f\"Rank {rank} processed {d['total_processed_rank']} tokens.\")\n",
    "    total_processed_global += d[\"total_processed_rank\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine results from all ranks\n",
    "eigenvalue_corrections = TensorDict({})\n",
    "\n",
    "\n",
    "for rank in range(workers):\n",
    "    eigenvalue_correction_test_path_rank = os.path.join(eigenvalue_correction_test_path, f\"rank_{rank}\")\n",
    "\n",
    "    # TensorDict wrapper to simplify tensor operations over dicts of tensors\n",
    "    eigenvalue_corrections_rank = TensorDict(\n",
    "        load_file(os.path.join(eigenvalue_correction_test_path_rank, \"eigenvalue_corrections.safetensors\"))\n",
    "    ).to(device)\n",
    "\n",
    "    if not eigenvalue_corrections:\n",
    "        eigenvalue_corrections = eigenvalue_corrections_rank\n",
    "    else:\n",
    "        eigenvalue_corrections = eigenvalue_corrections + (eigenvalue_corrections_rank)\n",
    "\n",
    "eigenvalue_corrections.div_(total_processed_global)\n",
    "save_file(\n",
    "    eigenvalue_corrections.to_dict(),\n",
    "    os.path.join(eigenvalue_correction_test_path, \"eigenvalue_corrections.safetensors\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "d_length = 10\n",
    "test_activations = {}\n",
    "test_gradients = {}\n",
    "batch_sizes = torch.randint(4000, 8000, (d_length,))\n",
    "activation_length = torch.randint(3000, 5000, (d_length,))\n",
    "gradient_length = torch.randint(3000, 10000, (d_length,))\n",
    "# Create test activations\n",
    "for i in range(d_length):\n",
    "    test_activations[f\"layer_{i}\"] = torch.rand(batch_sizes[i].item(), activation_length[i].item())\n",
    "    test_gradients[f\"layer_{i}\"] = torch.rand(batch_sizes[i].item(), gradient_length[i].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TensorDict(test_activations).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEBUG ZONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "dtype = torch.float32\n",
    "n, s, b = 10, 23, 30\n",
    "x = torch.empty((n, s, b), device=device, dtype=dtype)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
